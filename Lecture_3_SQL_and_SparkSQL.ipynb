{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>SQL and SparkSQL</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Overview</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "    <li>SQL Basics</li>\n",
    "    <li>SparkSQL - Context</li>\n",
    "    <li>JDBC Connectors</li>\n",
    "    <li>Spark Runtime</li>\n",
    "    <li>Basic Data Management Operations in SparkSQL</li>\n",
    "    <li>Take home excercise (Ungraded)</li>\n",
    "    <li>Readings</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Do you remember the Machine Learning Modeling Process that was introduced in our first lecture?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Machine learning has much more to it than just the traiing.\n",
    "\n",
    "<center><figure><img src=\"http://www.andrew.cmu.edu/~mfarag/static/mlmodeling.png\"/><figcaption>The Machine Learning Modeling Process</figcaption></figure></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>1. The Big Picture - Data Engineering Pipeline</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The big picture in machine learning modeling lies in the **Data Engineering pipeline concept**.\n",
    "A data pipeline consists of a series of connected processes that move the data from one point to another, possibly transforming the data along the way. **The data engineering pipeline covers up to the Data preparation phase of the machine learning modeling process.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><figure><img src=\"http://www.andrew.cmu.edu/~mfarag/static/overall_picture.png\"/><figcaption>The General Processes in Data Engineering Pipeline</figcaption></figure></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ingested data can be stored in a better format by being stored into:\n",
    "<ul>\n",
    "<li>Data warehouses. </li>\n",
    "<li>Data lakes. </li>\n",
    "<li>NoSQL databases.</li>\n",
    "<li>Relational databases.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our course, we will use the following pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><figure><img src=\"http://www.andrew.cmu.edu/~mfarag/static/class_tools.png\"/></figure></center>\n",
    "<b>We will discuss each step in detail throughout the course</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Relational Databases - One Option to Store Your Ingested Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "<li>A relational database (or RDB) is a structure for organizing information that one may think of initially as a supercharged version of a data frame.</li>\n",
    "<li>RDBs are commonly manipulated and queried using <b>SQL: Structured Query Language.</b></li>\n",
    "    <li>SQL is case insensitive</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul>\n",
    "    <li>In an RDB, the basic unit of data storage is the <b>table (or relation).</b> Rows are rows, but sometimes dubbed tuples or records, and columns are sometimes dubbed attributes. (Fields are specific cells within a given row.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Postgres</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>There are many implementations of SQL (SEE-kwule): Oracle, MySQL, SQLite, etc.</li>\n",
    "\n",
    "<li>In this class, we will make use of PostgreSQL, or more simply Postgres. It is open source, with Mac, Windows, and Linux versions available. You should download and install it!</li>\n",
    "\n",
    "<li>Postgres will set up a server on your machine. When you install Postgres, a superuser account “postgres” will be set up, with a password specified by you. Make this a good one, i.e., not just “postgres.” (That’s because hackers like to look for lazily set up postgres accounts.)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Categories of SQL Commands</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><figure><img src=\"https://www.andrew.cmu.edu/user/mfarag/static/sql-command-types.png\"/></figure><b>SQL Command Categories</b></center>\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Data Definition Language (DDL)</h2>\n",
    "DDL takes care of table structures. We will look into the most popular DDLs for creating, updating or dropping a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>1. Creating a Table</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Assuming you will need to create and populate a table to host your data.\n",
    "\n",
    "The command is\n",
    "\n",
    "<div style=\"background-color: #cfc ; padding: 10px; border: 1px solid green;\">\n",
    "    CREATE TABLE IF NOT EXISTS &lt; &lt; name &gt; &gt; <br/>\n",
    "(&lt;&lt; column 1 &gt;&gt;&gt; &lt;&lt; type 1 &gt;&gt; &lt;&lt; constraint 1 &gt;&gt;, ... , &lt;&lt; multi-column constraint(s) &gt;&gt;);   \n",
    "    </div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a simple example:   \n",
    "    \n",
    " <div style=\"background-color: lightgrey ; padding: 10px; border: 1px solid green;\">\n",
    " CREATE TABLE IF NOT EXISTS products( <br/>\n",
    " <div style=\"padding: 10px;\">\n",
    "    product_id SERIAL,<br/>\n",
    "  label TEXT,<br/>\n",
    "  price decimal,<br/>\n",
    "  inventory INTEGER<br/>\n",
    "    </div> \n",
    "     );</div>\n",
    "  \n",
    "  This creates a four-column table that contains a label for each product, its price, and the current number of each product available."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&#9432; (Be careful! You may not have the permissions to create tables - depending on your Postgres Installation-). To fix it, grant your user the proper permissions using this command <b>GRANT CREATE ON SCHEMA public TO postgres;</b><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><a href=\"https://www.postgresql.org/docs/current/datatype.html\">Available Data Types</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>2. Alter a Table</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you wish to add or delete an entire column, or rename an column, or change constraints, etc., you would “update” your table using alter table.\n",
    "\n",
    "The command is\n",
    "\n",
    "<div style=\"background-color: #cfc ; padding: 10px; border: 1px solid green;\">\n",
    "    ALTER TABLE &lt; &lt; name &gt; &gt; <br/>\n",
    "&lt;&lt; action &gt;&gt; ;\n",
    "    </div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are two simple examples:   \n",
    "    \n",
    " <div style=\"background-color: lightgrey ; padding: 10px; border: 1px solid green;\">\n",
    " ALTER TABLE products ADD COLUMN rating REAL DEFAULT 0.0;<br/>\n",
    "</div>\n",
    "    \n",
    " <div style=\"background-color: lightgrey ; padding: 10px; border: 1px solid green;\">\n",
    " ALTER TABLE products DROP COLUMN rating;\n",
    "    </div>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>3. Drop a Table</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To remove a table in its entirety:\n",
    "<div style=\"background-color: #cfc ; padding: 10px; border: 1px solid green;\">\n",
    "DROP TABLE &lt;&lt;name&gt;&gt;;\n",
    "    </div><br/>\n",
    "To check that the table is removed, look for it in the Table list on PgAdmin4."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Data Manipulation Language (DML)</h2>\n",
    "DML takes care of data in the tables.\n",
    "We will look into the most popular DMLs for data management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>1. Insert Values</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are a few ways to insert data into a SQL table. We’ll show how to do it row-by-row here, and then utilize select to create bigger tables next week.\n",
    "\n",
    "To populate the table one row at a time:\n",
    "\n",
    " <div style=\"background-color: #cfc ; padding: 10px; border: 1px solid green;\">\n",
    "INSERT INTO &lt;&lt; table &gt;&gt; (&lt;&lt; column i &gt;&gt;, &lt;&lt; column j &gt;&gt;,...) VALUES <br/>\n",
    "  (&lt;&lt; value i &gt;&gt;, &lt;&lt; value j &gt;&gt;,...),<br/>\n",
    "  ...<br/>\n",
    "  (&lt;&lt; value i &gt;&gt;, &lt;&lt; value j &gt;&gt;,...);<br/>\n",
    "    </div>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you leave out a column, then the data there will be missing (and can be added later with <b>UPDATE TABLE</b>) or will have a default value. Note that any column with data type SERIAL has default behavior: it will auto-increment).\n",
    "\n",
    " <div style=\"background-color: #cfc ; padding: 10px; border: 1px solid green;\">\n",
    "INSERT INTO products (label,price,inventory) VALUES <br/>\n",
    "   <div style=\"padding-left: 10px;\">\n",
    "    ('kirk action figure',50,13),<br/>\n",
    "  ('spock action figure',40,22);    \n",
    "</div>\n",
    "</div>\n",
    "The <i>product_id</i>, being of type SERIAL, will take on the values 1, then 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>2. Select: Querying a Database</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The <b>SELECT</b> command is how we query a database. It is a versatile and powerful command!<br/>\n",
    "\n",
    "A shortened definition that highlights elements of the syntax that are important in the context of this class is:\n",
    "\n",
    "<div style=\"background-color: #cfc ; padding: 10px; border: 1px solid green;\">\n",
    "SELECT <br/>\n",
    "    <div style=\"padding: 10px;\">\n",
    "  &lt;&lt;column1, column2, columnn&gt;&gt;<br/>\n",
    "  FROM &lt;&lt;table&gt;&gt; <br/>\n",
    "  [WHERE &lt;&lt;condition&gt;&gt;] <br/>\n",
    "  [GROUP BY &lt;&lt;expression&gt;&gt;] <br/>\n",
    "  [HAVING &lt;&lt;condition&gt;&gt;] <br/>\n",
    "  [ORDER BY &lt;&lt;expression&gt;&gt;]; <br/>\n",
    "        </div>\n",
    "        </div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Example:\n",
    "\n",
    "\n",
    " <div style=\"background-color: lightgrey ; padding: 10px; border: 1px solid green;\">\n",
    "SELECT product_id, label, price <br/> \n",
    "FROM products <br/>\n",
    " WHERE price >= 10;<br/>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>3. Update Table Values</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The <b>UPDATE</b> command allows us to modify values in table cells.\n",
    " <div style=\"background-color: #cfc ; padding: 10px; border: 1px solid green;\">\n",
    "UPDATE &lt;&lt; table &gt;&gt; <br/>\n",
    "         <div style=\"padding-left: 10px;\">\n",
    "   SET &lt;&lt;column1&gt;&gt; = &lt;&lt; new value 1 &gt;&gt; , <br/>\n",
    "   SET &lt;&lt;column2&gt;&gt; = &lt;&lt; new value 2 &gt;&gt;, <br/>\n",
    "   ... <br/>\n",
    "   WHERE &lt;&lt;row condition &gt;&gt;;\n",
    "             </div>\n",
    "             </div>\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Think of the <b>WHERE &lt; row condition &gt;</b> as being like a call to the <b>which()</b> function in R: in it, you set a range of values for one of the table columns, and thereby select which rows to update.\n",
    "             \n",
    "<div style=\"background-color: #cfc ; padding: 10px; border: 1px solid green;\">\n",
    "UPDATE products \n",
    "                           <div style=\"padding-left: 10px;\">\n",
    "                               set price = 100 <br/>\n",
    "                               where price &gt;= 45;\n",
    "                  </div>\n",
    "             </div>\n",
    "             \n",
    "(Note that when you look at an updated table, the serial data may not be displayed in numeric order, i.e., the rows may be rearranged. This is OK.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>4. Delete Rows</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To remove one or more entries from a table:\n",
    "<div style=\"background-color: #cfc ; padding: 10px; border: 1px solid green;\">\n",
    "DELETE FROM &lt;&lt; table &gt;&gt; <br/>\n",
    "    <div style=\"padding-left: 10px;\">\n",
    "    WHERE &lt;&lt; condition &gt;&gt;;\n",
    "    </div>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>SparkSQL - Context</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>SQL provides the main knowledge to ingest data into PostgreSQL tables. \n",
    "</li>\n",
    "<li>SQL doesn't support big-data processing on its own. So, in order to conduct SQL on big data, one good way to do it is to use Spark SQL</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>General Rule in SQL and SparkSQL Processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<h3>1. Use plain SQL to conduct DDL and DCL related operations and store them in *.sql files. These operations are not impacted by the data size</h3> \n",
    "<h3>2. Use SparkSQL to conduct DML related operations.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, Spark uses \"connectors\" to integrate its operation with external platforms/applications (e.g. PostgreSQL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><figure><img src=\"http://stat.cmu.edu/~mfarag/14810/l5/spark_integration.png\"/></figure></center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Install JDBC Connector</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "<li>Install the JDBC driver that matches your Java Version from <a href=\"https://jdbc.postgresql.org/download/\">https://jdbc.postgresql.org/download/</a></li>\n",
    "    <li>Place the installed  <b>JAR file</b> into your <b>SPARK_HOME directory under jars folder</b>.</li>\n",
    "    <li>Close all the terminals and restart Jupyter notebook over in a new terminal (or Restart Your Machine)</li></ul>\n",
    "<center><figure><img src=\"https://www.andrew.cmu.edu/user/mfarag/static/pg.png\"/></figure></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>The Non-SparkSQL way to Initialize the Application</h2>\n",
    "Continue to use this approach if you don't need to interface with Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the following lines if you are using Windows!\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "#findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('SparkTest').getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Spark Runtime</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "<li> SparkContext is used to help you access hardware-level and some software-level configurations for your Spark application.</li>\n",
    "    <li>Most Spark Components have their own Contexts (e.g. StreamingContext for streaming purposes).</li>\n",
    "    <li>Starting Spark 2.x, SparkSession was created to run applications in an easier way than SparkContext.</li>\n",
    "    <li>You may want to access the context object in order to specify the hardware-level or software-level configurations you need to specify</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><figure><img src=\"https://www.andrew.cmu.edu/user/mfarag/14813/sparksession.png\"/><figcaption>Contexts and SparkSession</figcaption></figure></center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Initialize SparkSQL Application - Create SQL Context</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the following lines if you are using Windows!\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "#findspark.find()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "appName = \"Big Data Analytics\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    "    .set('spark.driver.host','127.0.0.1')\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "# Create Spark Context with the new configurations rather than relying on the default one\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# You need to create SQL Context to conduct some database operations like what we will see later.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# If you have SQL context, you create the session from the Spark Context\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Download the Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!python -m wget https://www.andrew.cmu.edu/user/mfarag/763/KDDTrain+.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Read-in the Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from csv to a dataframe on a local machine. \n",
    "# header=False means the first row is not a header \n",
    "# sep=',' means the column are seperated using ','\n",
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"classes\",\"difficulty_level\"]\n",
    "\n",
    "df = spark.read.csv(\"KDDTrain+.txt\",header=False, inferSchema= True).toDF(*col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Print Schema</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: integer (nullable = true)\n",
      " |-- dst_bytes: integer (nullable = true)\n",
      " |-- land: integer (nullable = true)\n",
      " |-- wrong_fragment: integer (nullable = true)\n",
      " |-- urgent: integer (nullable = true)\n",
      " |-- hot: integer (nullable = true)\n",
      " |-- num_failed_logins: integer (nullable = true)\n",
      " |-- logged_in: integer (nullable = true)\n",
      " |-- num_compromised: integer (nullable = true)\n",
      " |-- root_shell: integer (nullable = true)\n",
      " |-- su_attempted: integer (nullable = true)\n",
      " |-- num_root: integer (nullable = true)\n",
      " |-- num_file_creations: integer (nullable = true)\n",
      " |-- num_shells: integer (nullable = true)\n",
      " |-- num_access_files: integer (nullable = true)\n",
      " |-- num_outbound_cmds: integer (nullable = true)\n",
      " |-- is_host_login: integer (nullable = true)\n",
      " |-- is_guest_login: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- srv_count: integer (nullable = true)\n",
      " |-- serror_rate: double (nullable = true)\n",
      " |-- srv_serror_rate: double (nullable = true)\n",
      " |-- rerror_rate: double (nullable = true)\n",
      " |-- srv_rerror_rate: double (nullable = true)\n",
      " |-- same_srv_rate: double (nullable = true)\n",
      " |-- diff_srv_rate: double (nullable = true)\n",
      " |-- srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_count: integer (nullable = true)\n",
      " |-- dst_host_srv_count: integer (nullable = true)\n",
      " |-- dst_host_same_srv_rate: double (nullable = true)\n",
      " |-- dst_host_diff_srv_rate: double (nullable = true)\n",
      " |-- dst_host_same_src_port_rate: double (nullable = true)\n",
      " |-- dst_host_srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_serror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_serror_rate: double (nullable = true)\n",
      " |-- dst_host_rerror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_rerror_rate: double (nullable = true)\n",
      " |-- classes: string (nullable = true)\n",
      " |-- difficulty_level: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Write Data to Tables in SparkSQL</h2>\n",
    "What is the SQL command represented in this code snippet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# update the options with your db user and password\n",
    "\n",
    "db_properties={}\n",
    "db_properties['username']=\"postgres\"\n",
    "db_properties['password']=\"bigdata\"\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "db_properties['table']=\"intrusion\"\n",
    "db_properties['driver']=\"org.postgresql.Driver\"\n",
    "\n",
    "df.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "# create Spark context with necessary configuration\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "sc.textFile(\"inputFile.txt\").flatMap(lambda x: x.split());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the options with your db user and password\n",
    "\n",
    "db_properties={}\n",
    "db_properties['username']=\"postgres\"\n",
    "db_properties['password']=\"bigdata\"\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "db_properties['table']=\"xyz\"\n",
    "db_properties['driver']=\"org.postgresql.Driver\"\n",
    "\n",
    "df.write.format(\"jdbc\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()\n",
    "\n",
    "df.write.format(\"jdbc\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"jdbc\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Spark Data Write Modes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In Spark, there are several modes for writing your dataframe data to external data source. In the previous example, we used \"overwrite\" but it's not the only option!\n",
    "<ul>\n",
    "    <li><b>append:</b> Append contents of this dataframe to existing table's data. <b>Append</b> simulates the process of <b>inserting new records.</b></li>\n",
    "<li><b>overwrite:</b> Overwrite existing data. It recreates the table with all the data. Use it with cautious.</li>\n",
    "<li><b>ignore:</b> Silently ignore this operation if data already exists.</li>\n",
    "<li><b>error</b> (default case): Throw an exception if data already exists.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Now, read the data back!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " duration                    | 0      \n",
      " protocol_type               | tcp    \n",
      " service                     | http   \n",
      " flag                        | REJ    \n",
      " src_bytes                   | 0      \n",
      " dst_bytes                   | 0      \n",
      " land                        | 0      \n",
      " wrong_fragment              | 0      \n",
      " urgent                      | 0      \n",
      " hot                         | 0      \n",
      " num_failed_logins           | 0      \n",
      " logged_in                   | 0      \n",
      " num_compromised             | 0      \n",
      " root_shell                  | 0      \n",
      " su_attempted                | 0      \n",
      " num_root                    | 0      \n",
      " num_file_creations          | 0      \n",
      " num_shells                  | 0      \n",
      " num_access_files            | 0      \n",
      " num_outbound_cmds           | 0      \n",
      " is_host_login               | 0      \n",
      " is_guest_login              | 0      \n",
      " count                       | 1      \n",
      " srv_count                   | 1      \n",
      " serror_rate                 | 0.0    \n",
      " srv_serror_rate             | 0.0    \n",
      " rerror_rate                 | 1.0    \n",
      " srv_rerror_rate             | 1.0    \n",
      " same_srv_rate               | 1.0    \n",
      " diff_srv_rate               | 0.0    \n",
      " srv_diff_host_rate          | 0.0    \n",
      " dst_host_count              | 34     \n",
      " dst_host_srv_count          | 251    \n",
      " dst_host_same_srv_rate      | 1.0    \n",
      " dst_host_diff_srv_rate      | 0.0    \n",
      " dst_host_same_src_port_rate | 0.03   \n",
      " dst_host_srv_diff_host_rate | 0.06   \n",
      " dst_host_serror_rate        | 0.0    \n",
      " dst_host_srv_serror_rate    | 0.0    \n",
      " dst_host_rerror_rate        | 1.0    \n",
      " dst_host_srv_rerror_rate    | 0.98   \n",
      " classes                     | normal \n",
      " difficulty_level            | 21     \n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: integer (nullable = true)\n",
      " |-- dst_bytes: integer (nullable = true)\n",
      " |-- land: integer (nullable = true)\n",
      " |-- wrong_fragment: integer (nullable = true)\n",
      " |-- urgent: integer (nullable = true)\n",
      " |-- hot: integer (nullable = true)\n",
      " |-- num_failed_logins: integer (nullable = true)\n",
      " |-- logged_in: integer (nullable = true)\n",
      " |-- num_compromised: integer (nullable = true)\n",
      " |-- root_shell: integer (nullable = true)\n",
      " |-- su_attempted: integer (nullable = true)\n",
      " |-- num_root: integer (nullable = true)\n",
      " |-- num_file_creations: integer (nullable = true)\n",
      " |-- num_shells: integer (nullable = true)\n",
      " |-- num_access_files: integer (nullable = true)\n",
      " |-- num_outbound_cmds: integer (nullable = true)\n",
      " |-- is_host_login: integer (nullable = true)\n",
      " |-- is_guest_login: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- srv_count: integer (nullable = true)\n",
      " |-- serror_rate: double (nullable = true)\n",
      " |-- srv_serror_rate: double (nullable = true)\n",
      " |-- rerror_rate: double (nullable = true)\n",
      " |-- srv_rerror_rate: double (nullable = true)\n",
      " |-- same_srv_rate: double (nullable = true)\n",
      " |-- diff_srv_rate: double (nullable = true)\n",
      " |-- srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_count: integer (nullable = true)\n",
      " |-- dst_host_srv_count: integer (nullable = true)\n",
      " |-- dst_host_same_srv_rate: double (nullable = true)\n",
      " |-- dst_host_diff_srv_rate: double (nullable = true)\n",
      " |-- dst_host_same_src_port_rate: double (nullable = true)\n",
      " |-- dst_host_srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_serror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_serror_rate: double (nullable = true)\n",
      " |-- dst_host_rerror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_rerror_rate: double (nullable = true)\n",
      " |-- classes: string (nullable = true)\n",
      " |-- difficulty_level: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()\n",
    "\n",
    "df_read.show(1, vertical=True)\n",
    "df_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Selecting Columns</h3>\n",
    "Keep in mind, you can chain functions here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------\n",
      " protocol_type | tcp    \n",
      " duration      | 0      \n",
      " flag          | SF     \n",
      " classes       | normal \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting a single column\n",
    "subset_df = df.select(\"classes\")\n",
    "\n",
    "# Selecting multiple columns and display them on the flyabs\n",
    "df.select(\"protocol_type\",\"duration\", \"flag\",\"classes\").show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Filtering Data</h3>\n",
    "Filtering allows you to subset a DataFrame based on specific conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " duration                    | 0      \n",
      " protocol_type               | tcp    \n",
      " service                     | http   \n",
      " flag                        | REJ    \n",
      " src_bytes                   | 0      \n",
      " dst_bytes                   | 0      \n",
      " land                        | 0      \n",
      " wrong_fragment              | 0      \n",
      " urgent                      | 0      \n",
      " hot                         | 0      \n",
      " num_failed_logins           | 0      \n",
      " logged_in                   | 0      \n",
      " num_compromised             | 0      \n",
      " root_shell                  | 0      \n",
      " su_attempted                | 0      \n",
      " num_root                    | 0      \n",
      " num_file_creations          | 0      \n",
      " num_shells                  | 0      \n",
      " num_access_files            | 0      \n",
      " num_outbound_cmds           | 0      \n",
      " is_host_login               | 0      \n",
      " is_guest_login              | 0      \n",
      " count                       | 1      \n",
      " srv_count                   | 1      \n",
      " serror_rate                 | 0.0    \n",
      " srv_serror_rate             | 0.0    \n",
      " rerror_rate                 | 1.0    \n",
      " srv_rerror_rate             | 1.0    \n",
      " same_srv_rate               | 1.0    \n",
      " diff_srv_rate               | 0.0    \n",
      " srv_diff_host_rate          | 0.0    \n",
      " dst_host_count              | 34     \n",
      " dst_host_srv_count          | 251    \n",
      " dst_host_same_srv_rate      | 1.0    \n",
      " dst_host_diff_srv_rate      | 0.0    \n",
      " dst_host_same_src_port_rate | 0.03   \n",
      " dst_host_srv_diff_host_rate | 0.06   \n",
      " dst_host_serror_rate        | 0.0    \n",
      " dst_host_srv_serror_rate    | 0.0    \n",
      " dst_host_rerror_rate        | 1.0    \n",
      " dst_host_srv_rerror_rate    | 0.98   \n",
      " classes                     | normal \n",
      " difficulty_level            | 21     \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where a condition is met\n",
    "df_read.filter(df_read[\"classes\"] == 'normal').show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Filter using SQL-like syntax\n",
    "df_read.filter(\"count > 1\").show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Renaming Columns</h3>\n",
    "To rename columns in a DataFrame, you can use the withColumnRenamed() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- source_bytes: integer (nullable = true)\n",
      " |-- dst_bytes: integer (nullable = true)\n",
      " |-- land: integer (nullable = true)\n",
      " |-- wrong_fragment: integer (nullable = true)\n",
      " |-- urgent: integer (nullable = true)\n",
      " |-- hot: integer (nullable = true)\n",
      " |-- num_failed_logins: integer (nullable = true)\n",
      " |-- logged_in: integer (nullable = true)\n",
      " |-- num_compromised: integer (nullable = true)\n",
      " |-- root_shell: integer (nullable = true)\n",
      " |-- su_attempted: integer (nullable = true)\n",
      " |-- num_root: integer (nullable = true)\n",
      " |-- num_file_creations: integer (nullable = true)\n",
      " |-- num_shells: integer (nullable = true)\n",
      " |-- num_access_files: integer (nullable = true)\n",
      " |-- num_outbound_cmds: integer (nullable = true)\n",
      " |-- is_host_login: integer (nullable = true)\n",
      " |-- is_guest_login: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- srv_count: integer (nullable = true)\n",
      " |-- serror_rate: double (nullable = true)\n",
      " |-- srv_serror_rate: double (nullable = true)\n",
      " |-- rerror_rate: double (nullable = true)\n",
      " |-- srv_rerror_rate: double (nullable = true)\n",
      " |-- same_srv_rate: double (nullable = true)\n",
      " |-- diff_srv_rate: double (nullable = true)\n",
      " |-- srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_count: integer (nullable = true)\n",
      " |-- dst_host_srv_count: integer (nullable = true)\n",
      " |-- dst_host_same_srv_rate: double (nullable = true)\n",
      " |-- dst_host_diff_srv_rate: double (nullable = true)\n",
      " |-- dst_host_same_src_port_rate: double (nullable = true)\n",
      " |-- dst_host_srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_serror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_serror_rate: double (nullable = true)\n",
      " |-- dst_host_rerror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_rerror_rate: double (nullable = true)\n",
      " |-- classes: string (nullable = true)\n",
      " |-- difficulty_level: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where a condition is met\n",
    "df_read.withColumnRenamed(\"src_bytes\", \"source_bytes\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Transforming Data</h3>\n",
    "Data transformation involves applying functions or expressions to columns in a DataFrame to create new columns or modify existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " duration                    | 0      \n",
      " protocol_type               | tcp    \n",
      " service                     | http   \n",
      " flag                        | REJ    \n",
      " src_bytes                   | 0      \n",
      " dst_bytes                   | 0      \n",
      " land                        | 0      \n",
      " wrong_fragment              | 0      \n",
      " urgent                      | 0      \n",
      " hot                         | 0      \n",
      " num_failed_logins           | 0      \n",
      " logged_in                   | 0      \n",
      " num_compromised             | 0      \n",
      " root_shell                  | 0      \n",
      " su_attempted                | 0      \n",
      " num_root                    | 0      \n",
      " num_file_creations          | 0      \n",
      " num_shells                  | 0      \n",
      " num_access_files            | 0      \n",
      " num_outbound_cmds           | 0      \n",
      " is_host_login               | 0      \n",
      " is_guest_login              | 0      \n",
      " count                       | 1      \n",
      " srv_count                   | 1      \n",
      " serror_rate                 | 0.0    \n",
      " srv_serror_rate             | 0.0    \n",
      " rerror_rate                 | 1.0    \n",
      " srv_rerror_rate             | 1.0    \n",
      " same_srv_rate               | 1.0    \n",
      " diff_srv_rate               | 0.0    \n",
      " srv_diff_host_rate          | 0.0    \n",
      " dst_host_count              | 34     \n",
      " dst_host_srv_count          | 251    \n",
      " dst_host_same_srv_rate      | 1.0    \n",
      " dst_host_diff_srv_rate      | 0.0    \n",
      " dst_host_same_src_port_rate | 0.03   \n",
      " dst_host_srv_diff_host_rate | 0.06   \n",
      " dst_host_serror_rate        | 0.0    \n",
      " dst_host_srv_serror_rate    | 0.0    \n",
      " dst_host_rerror_rate        | 1.0    \n",
      " dst_host_srv_rerror_rate    | 0.98   \n",
      " classes                     | normal \n",
      " difficulty_level            | 21     \n",
      " double_difficulty_level     | 42     \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a new column\n",
    "df_with_double_difficulty_level = df_read.withColumn(\"double_difficulty_level\", col(\"difficulty_level\") * 2)\n",
    "\n",
    "df_with_double_difficulty_level.show(1,vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Using expressions\n",
    "df_with_new_count = df_read.withColumn(\"new_count\", expr(\"count + 1\"))\n",
    "\n",
    "df_with_new_count.show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Aggregating Data</h3>\n",
    "Aggregation operations are essential for summarizing data. PySpark provides various aggregation functions like sum(), avg(), min(), and max()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|      avg(count)|\n",
      "+----------------+\n",
      "|84.1075547934875|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Create a new column\n",
    "df_read.select(avg(\"count\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Group By Example Using SparkSQL Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|protocol_type| count|\n",
      "+-------------+------+\n",
      "|          tcp|102689|\n",
      "|          udp| 14993|\n",
      "|         icmp|  8291|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read.groupby(\"protocol_type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Select: Order By</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that one can use an ordinal number (as in <b>GROUP BY 1</b>, meaning group by the first column of the selected table). Also note that the output from GroupBy is not necessarily ordered by any column (the output is in essentially random order)…that’s what <b>ORDER BY</b> is for.\n",
    "\n",
    "<b>The ASC means “ascending,” which is the default, which is contrasted with DESC for \"descending.\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " duration                    | 0      \n",
      " protocol_type               | tcp    \n",
      " service                     | smtp   \n",
      " flag                        | SF     \n",
      " src_bytes                   | 2416   \n",
      " dst_bytes                   | 334    \n",
      " land                        | 0      \n",
      " wrong_fragment              | 0      \n",
      " urgent                      | 0      \n",
      " hot                         | 0      \n",
      " num_failed_logins           | 0      \n",
      " logged_in                   | 1      \n",
      " num_compromised             | 0      \n",
      " root_shell                  | 0      \n",
      " su_attempted                | 0      \n",
      " num_root                    | 0      \n",
      " num_file_creations          | 0      \n",
      " num_shells                  | 0      \n",
      " num_access_files            | 0      \n",
      " num_outbound_cmds           | 0      \n",
      " is_host_login               | 0      \n",
      " is_guest_login              | 0      \n",
      " count                       | 1      \n",
      " srv_count                   | 1      \n",
      " serror_rate                 | 0.0    \n",
      " srv_serror_rate             | 0.0    \n",
      " rerror_rate                 | 0.0    \n",
      " srv_rerror_rate             | 0.0    \n",
      " same_srv_rate               | 1.0    \n",
      " diff_srv_rate               | 0.0    \n",
      " srv_diff_host_rate          | 0.0    \n",
      " dst_host_count              | 247    \n",
      " dst_host_srv_count          | 147    \n",
      " dst_host_same_srv_rate      | 0.57   \n",
      " dst_host_diff_srv_rate      | 0.02   \n",
      " dst_host_same_src_port_rate | 0.0    \n",
      " dst_host_srv_diff_host_rate | 0.01   \n",
      " dst_host_serror_rate        | 0.0    \n",
      " dst_host_srv_serror_rate    | 0.0    \n",
      " dst_host_rerror_rate        | 0.0    \n",
      " dst_host_srv_rerror_rate    | 0.0    \n",
      " classes                     | normal \n",
      " difficulty_level            | 21     \n",
      "-RECORD 1-----------------------------\n",
      " duration                    | 0      \n",
      " protocol_type               | tcp    \n",
      " service                     | http   \n",
      " flag                        | SF     \n",
      " src_bytes                   | 210    \n",
      " dst_bytes                   | 2593   \n",
      " land                        | 0      \n",
      " wrong_fragment              | 0      \n",
      " urgent                      | 0      \n",
      " hot                         | 0      \n",
      " num_failed_logins           | 0      \n",
      " logged_in                   | 1      \n",
      " num_compromised             | 0      \n",
      " root_shell                  | 0      \n",
      " su_attempted                | 0      \n",
      " num_root                    | 0      \n",
      " num_file_creations          | 0      \n",
      " num_shells                  | 0      \n",
      " num_access_files            | 0      \n",
      " num_outbound_cmds           | 0      \n",
      " is_host_login               | 0      \n",
      " is_guest_login              | 0      \n",
      " count                       | 3      \n",
      " srv_count                   | 3      \n",
      " serror_rate                 | 0.0    \n",
      " srv_serror_rate             | 0.0    \n",
      " rerror_rate                 | 0.0    \n",
      " srv_rerror_rate             | 0.0    \n",
      " same_srv_rate               | 1.0    \n",
      " diff_srv_rate               | 0.0    \n",
      " srv_diff_host_rate          | 0.0    \n",
      " dst_host_count              | 25     \n",
      " dst_host_srv_count          | 255    \n",
      " dst_host_same_srv_rate      | 1.0    \n",
      " dst_host_diff_srv_rate      | 0.0    \n",
      " dst_host_same_src_port_rate | 0.04   \n",
      " dst_host_srv_diff_host_rate | 0.05   \n",
      " dst_host_serror_rate        | 0.0    \n",
      " dst_host_srv_serror_rate    | 0.0    \n",
      " dst_host_rerror_rate        | 0.0    \n",
      " dst_host_srv_rerror_rate    | 0.0    \n",
      " classes                     | normal \n",
      " difficulty_level            | 21     \n",
      "-RECORD 2-----------------------------\n",
      " duration                    | 0      \n",
      " protocol_type               | tcp    \n",
      " service                     | http   \n",
      " flag                        | SF     \n",
      " src_bytes                   | 291    \n",
      " dst_bytes                   | 444    \n",
      " land                        | 0      \n",
      " wrong_fragment              | 0      \n",
      " urgent                      | 0      \n",
      " hot                         | 0      \n",
      " num_failed_logins           | 0      \n",
      " logged_in                   | 1      \n",
      " num_compromised             | 0      \n",
      " root_shell                  | 0      \n",
      " su_attempted                | 0      \n",
      " num_root                    | 0      \n",
      " num_file_creations          | 0      \n",
      " num_shells                  | 0      \n",
      " num_access_files            | 0      \n",
      " num_outbound_cmds           | 0      \n",
      " is_host_login               | 0      \n",
      " is_guest_login              | 0      \n",
      " count                       | 2      \n",
      " srv_count                   | 15     \n",
      " serror_rate                 | 0.0    \n",
      " srv_serror_rate             | 0.0    \n",
      " rerror_rate                 | 0.0    \n",
      " srv_rerror_rate             | 0.0    \n",
      " same_srv_rate               | 1.0    \n",
      " diff_srv_rate               | 0.0    \n",
      " srv_diff_host_rate          | 0.13   \n",
      " dst_host_count              | 4      \n",
      " dst_host_srv_count          | 40     \n",
      " dst_host_same_srv_rate      | 1.0    \n",
      " dst_host_diff_srv_rate      | 0.0    \n",
      " dst_host_same_src_port_rate | 0.25   \n",
      " dst_host_srv_diff_host_rate | 0.05   \n",
      " dst_host_serror_rate        | 0.0    \n",
      " dst_host_srv_serror_rate    | 0.0    \n",
      " dst_host_rerror_rate        | 0.0    \n",
      " dst_host_srv_rerror_rate    | 0.0    \n",
      " classes                     | normal \n",
      " difficulty_level            | 21     \n",
      "-RECORD 3-----------------------------\n",
      " duration                    | 0      \n",
      " protocol_type               | tcp    \n",
      " service                     | http   \n",
      " flag                        | SF     \n",
      " src_bytes                   | 308    \n",
      " dst_bytes                   | 4533   \n",
      " land                        | 0      \n",
      " wrong_fragment              | 0      \n",
      " urgent                      | 0      \n",
      " hot                         | 0      \n",
      " num_failed_logins           | 0      \n",
      " logged_in                   | 1      \n",
      " num_compromised             | 0      \n",
      " root_shell                  | 0      \n",
      " su_attempted                | 0      \n",
      " num_root                    | 0      \n",
      " num_file_creations          | 0      \n",
      " num_shells                  | 0      \n",
      " num_access_files            | 0      \n",
      " num_outbound_cmds           | 0      \n",
      " is_host_login               | 0      \n",
      " is_guest_login              | 0      \n",
      " count                       | 20     \n",
      " srv_count                   | 20     \n",
      " serror_rate                 | 0.0    \n",
      " srv_serror_rate             | 0.0    \n",
      " rerror_rate                 | 0.0    \n",
      " srv_rerror_rate             | 0.0    \n",
      " same_srv_rate               | 1.0    \n",
      " diff_srv_rate               | 0.0    \n",
      " srv_diff_host_rate          | 0.0    \n",
      " dst_host_count              | 79     \n",
      " dst_host_srv_count          | 255    \n",
      " dst_host_same_srv_rate      | 1.0    \n",
      " dst_host_diff_srv_rate      | 0.0    \n",
      " dst_host_same_src_port_rate | 0.01   \n",
      " dst_host_srv_diff_host_rate | 0.01   \n",
      " dst_host_serror_rate        | 0.0    \n",
      " dst_host_srv_serror_rate    | 0.0    \n",
      " dst_host_rerror_rate        | 0.0    \n",
      " dst_host_srv_rerror_rate    | 0.0    \n",
      " classes                     | normal \n",
      " difficulty_level            | 21     \n",
      "-RECORD 4-----------------------------\n",
      " duration                    | 0      \n",
      " protocol_type               | tcp    \n",
      " service                     | http   \n",
      " flag                        | SF     \n",
      " src_bytes                   | 318    \n",
      " dst_bytes                   | 1251   \n",
      " land                        | 0      \n",
      " wrong_fragment              | 0      \n",
      " urgent                      | 0      \n",
      " hot                         | 0      \n",
      " num_failed_logins           | 0      \n",
      " logged_in                   | 1      \n",
      " num_compromised             | 0      \n",
      " root_shell                  | 0      \n",
      " su_attempted                | 0      \n",
      " num_root                    | 0      \n",
      " num_file_creations          | 0      \n",
      " num_shells                  | 0      \n",
      " num_access_files            | 0      \n",
      " num_outbound_cmds           | 0      \n",
      " is_host_login               | 0      \n",
      " is_guest_login              | 0      \n",
      " count                       | 7      \n",
      " srv_count                   | 7      \n",
      " serror_rate                 | 0.0    \n",
      " srv_serror_rate             | 0.0    \n",
      " rerror_rate                 | 0.0    \n",
      " srv_rerror_rate             | 0.0    \n",
      " same_srv_rate               | 1.0    \n",
      " diff_srv_rate               | 0.0    \n",
      " srv_diff_host_rate          | 0.0    \n",
      " dst_host_count              | 59     \n",
      " dst_host_srv_count          | 255    \n",
      " dst_host_same_srv_rate      | 1.0    \n",
      " dst_host_diff_srv_rate      | 0.0    \n",
      " dst_host_same_src_port_rate | 0.02   \n",
      " dst_host_srv_diff_host_rate | 0.03   \n",
      " dst_host_serror_rate        | 0.0    \n",
      " dst_host_srv_serror_rate    | 0.0    \n",
      " dst_host_rerror_rate        | 0.0    \n",
      " dst_host_srv_rerror_rate    | 0.0    \n",
      " classes                     | normal \n",
      " difficulty_level            | 21     \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read.orderBy(\"difficulty_level\", ascending=False).show(5,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Try at home and enrich your knowledge</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "<li>Find the most popular 5 difficulty levels.</li>\n",
    "<li>Find the total number of records where the user was not logged in</li>\n",
    "<li>What is the most popular \"attack\" for TCP connections? (i.e., records with protocol_type = \"tcp\").</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Readings</h2>\n",
    "<ul>\n",
    "<li>Application of SQL Database, published on Canvas</li>\n",
    "<li>Applicationn of PySpark, published on Canvas</li>\n",
    "<li><a href=\"https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html\">PySpark Dataframe Guide</a></li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
